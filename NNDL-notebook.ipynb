{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ceed3b",
   "metadata": {},
   "source": [
    "##### Project ID: 028 \n",
    "\n",
    "##### Project Title: Environmental Sound Classification \n",
    "\n",
    "##### Area of Research: Computer Vision\n",
    "\n",
    "##### Team name: NNDL\n",
    "\n",
    "##### Team members:\n",
    "\n",
    "Sheng Zhang(z5446399)\n",
    "\n",
    "Zelong Huang(z5489331) \n",
    "\n",
    "Shanyu Zhou(z5466581) \n",
    "\n",
    "Lingyun Yan(z5467937)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dbb4b",
   "metadata": {},
   "source": [
    "### 1. Introduction, Motivation and Problem Statement\n",
    "#### 1.1 Introduction\n",
    "<img src=\"./picture/esc50.gif\" alt=\"final\" width=\"600\">\n",
    "\n",
    "In today's information age, the field of machine learning has seen rapid technological advancement and significant achievements in various fields, although research in speech and music processing has progressed, the analysis of ambient sound is still relatively lagging behind. Environmental Sound Classification (ESC), as one of the important application areas, is attracting more and more researchers' attention. \n",
    "\n",
    "Environmental sound refers to a variety of non-speech sounds produced by nature and human activities, such as animal calls, traffic noise, natural environmental sounds (e.g., rain, wind), and home environmental sounds such as washing machine sound, door bell. These sounds contain a wealth of information and provide important clues about the state of the environment, the type of activity, and so on.\n",
    "\n",
    "#### 1.2 Motivation\n",
    "With the rapid development of deep learning technology, which has made breakthroughs in the fields of image, speech and natural language processing, ambient sound classification has a wide range of application prospects and important significance in the real world. Ambient sound classification technology is widely used in many fields, such as audio surveillance systems to detect and respond to security threats in a timely manner, hearing aids using sound classification technology to enhance the user's understanding of ambient sound, smart homes where sound classification is used for home security monitoring and user experience enhancement, video content generation where background sound effects can be added automatically, and automated driving to help vehicles recognize ambient sound signals to improve safety. \n",
    "\n",
    "In ecological monitoring, natural environmental sounds are analyzed to protect wildlife and identify environmental changes, and in medical diagnostics, sounds such as breathing and coughing are analyzed to assist doctors in early diagnosis and monitoring. For the research of this problem, this not only helps to promote the development of academic research, but also provides technical support and solutions for practical applications.\n",
    "\n",
    "#### 1.3 Problem Statement\n",
    "This study will address the problem of environmental sound classification, and the ESC-50 used provides a standardized benchmark dataset. In the task of environmental sound classification, the following problems are faced: the diversity and complexity of sound data, the influence of noise and sound quality of the environment, model selection and feature extraction, and generalization capabilities. \n",
    "\n",
    "Based on the above problems, the objectives of this study are: exploring and applying different deep learning models, optimizing feature extraction methods, and improving the robustness and generalization ability of the models. We hope that the study of the above problems can find the optimal strategy for environmental sound classification and provide valuable help for future research and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff20da4",
   "metadata": {},
   "source": [
    "### 2. Data Sources\n",
    "\n",
    "#### 2.1 Dataset Description\n",
    "The ESC-50 dataset is designed for the classification of environmental sounds. It contains a diverse range of audio clips representing various everyday sounds, it contains 2000 audio clips divided into 50 categories of 40 audio clips each. All audio clips are 5-second WAV files with a sampling rate of 44.1 kHz in mono.\n",
    "\n",
    "#### 2.2 Specific Categories\n",
    "This table lists the five major categories of the ESC-50 dataset and their corresponding subcategories.\n",
    "<img src=\"./picture/截屏2024-07-21 10.51.50.png\" alt=\"final\" width=\"800\">\n",
    "\n",
    "#### 2.3 Download\n",
    "The dataset can be downloaded from the following link：\n",
    "https://github.com/karoldvl/ESC-50/archive/master.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c174f8",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis of Data\n",
    "\n",
    "#### 3.1 Data properties\n",
    "**Data Type:** The data for this project consists of direct audio signals and images using converted audio. to facilitate processing using image recognition models such as Convolutional Neural Networks (CNN).\n",
    "\n",
    "#### 3.2 Number of categories\n",
    "**Number of categories:** The dataset contains 50 categories, each with 40 audio files.\n",
    "The project divides these audio files into dataset and test set, which are roughly categorized into 9:1, 8:2, 7:3 by looking at some common audio categorization papers.\n",
    "\n",
    "#### 3.3 Preprocessing\n",
    "##### 3.3.1 Audio signal preprocessing\n",
    "+ **Normalization:** Normalize the audio signal to improve the stability and performance of subsequent processing and analysis, and reduce the effect of noise.\n",
    "+ **Label Coding:** Use one-hot coding to convert labels to a categorical format to avoid sequential relationships between categories, improve model performance, and good compatibility.\n",
    "##### 3.3.2 Image Preprocessing\n",
    "+ **Gray Scale Processing:** Convert audio to gray scale image, it reduces computational complexity and storage requirements, reduces data redundancy, makes processing more concise and increases processing speed.\n",
    "+ **Size Adjustment:** At first, our group learned from the paper that 1000*400 size image has good effect, so we initially used that size image, but the performance is not good in the actual test, adjusted to 224*224, in the subsequent test to get better results.\n",
    "\n",
    "#### 3.4 Feature Extraction\n",
    "We have learned that feature extraction is roughly done in the following three ways by reviewing related papers.\n",
    "\n",
    "+ **STFT** \n",
    "\n",
    "STFT, full name Short-Time Fourier Transform, which is based on the principle of dividing the audio signal into small time segments, and applying Fourier Transform to the small time segments.STFT can capture the frequency characteristics of the sound in time, for example, in the ESC-50 dataset, STFT can capture the beginning and end of a dog barking, or the persistence pattern of the sound of rain.\n",
    "\n",
    "<img src=\"./picture/stft.png\" alt=\"final\" width=\"600\">\n",
    "\n",
    "+ **MFCC** \n",
    "\n",
    "MFCC, full name Mel Frequency Cepstral Coefficients, which maps the linear spectrum onto the Mel scale after performing a Fourier transform on the audio. In the ESC-50 dataset, MFCC can effectively capture the timbral characteristics of sounds, such as distinguishing different animal calls, or different types of machine sounds.\n",
    "\n",
    "<img src=\"./picture/mfcc.png\" alt=\"final\" width=\"600\">\n",
    "\n",
    "+ **Mel** \n",
    "\n",
    "The Mel spectrogram is the result of converting a regular spectrogram to the Mel frequency scale. It retains the time-frequency representation of STFT, but the frequency axes use the Meier scale, which is more compatible with the perception of the human ear. In the ESC-50 dataset, the Mel Spectrogram is effective in capturing time-frequency patterns of ambient sounds, such as the sustained high-frequency component of rain, or the transient high-energy signature of a car horn.\n",
    "\n",
    "<img src=\"./picture/mel.png\" alt=\"final\" width=\"600\">\n",
    "\n",
    "#### 3.5 Data Augmentation\n",
    "+ **Audio Data Enhancement:** Including time shifting, adding noise and other methods to improve the robustness and generalization ability of the model.\n",
    "+ **Image Data Enhancement:** Including rotation, horizontal flipping, adding noise and other methods to improve the robustness of the model and generalization ability.\n",
    "\n",
    "#### 3.6 Challenging aspects\n",
    "+ **Category diversity:** the 50 categories cover a wide range of sound types, which increases the difficulty of categorization. \n",
    "+ **Fewer samples per category:** there are only 40 samples per category, which can lead to overfitting or underfitting problems. \n",
    "+ **Background noise:** real-world recordings may contain background noise, which increases the complexity of classification. \n",
    "+ **Intra-class variation:** there may be significant variation in sounds from the same category, e.g. different kinds of dog barks. \n",
    "+ **Inter-class similarity:** there may be similarities between certain classes, e.g. \"rain\" and \"waves\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d80e7d",
   "metadata": {},
   "source": [
    "### 4. Basic Model\n",
    "We chose different models based on traditional machine learning direction, CNN direction, RNN direction, for different models respectively change their feature extraction method, image size, training set to test set ratio, optimizer selection, optimizer learning rate, training rounds and run out multiple sets of results for comparison.\n",
    "\n",
    "##### Evaluation of model results\n",
    "And we also output loss vs. rounds plots, confusion matrices, accuracy, f1-score, and runtime for each training to evaluate the model's results.\n",
    "\n",
    "For solving this problem, based on previous learning experiences with neural networks as well as machine learning, the first thing that comes to mind is to convert audio into pictures, and transform the problem of categorizing sounds into a problem of categorizing pictures.\n",
    "\n",
    "So we convert all the audio into images using different feature extraction methods, here is an example, we convert the audio into a Mel Spectrogram with a size of 224*224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aada452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def wav_to_melspectrogram_image(wav_file, output_folder):\n",
    "    y, sr = librosa.load(wav_file, sr=None)  # Load the audio file\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)  # Compute the Mel spectrogram\n",
    "    S_DB = librosa.power_to_db(S, ref=np.max)  # Convert to dB scale\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "    output_file = os.path.join(output_folder, f\"{os.path.splitext(os.path.basename(wav_file))[0]}.png\")\n",
    "    \n",
    "    # Plot and save the Mel spectrogram\n",
    "    fig, ax = plt.subplots(figsize=(2.24, 2.24), dpi=100)\n",
    "    ax.set_axis_off()\n",
    "    librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel', cmap='viridis', ax=ax)\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    fig.savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "def process_all_wav_files(input_folder, output_folder):\n",
    "    for wav_file in os.listdir(input_folder):\n",
    "        if wav_file.endswith('.wav'):\n",
    "            wav_to_melspectrogram_image(os.path.join(input_folder, wav_file), output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d199816",
   "metadata": {},
   "source": [
    "#### 4.1 Traditional machine learning\n",
    "For the problem of classifying audio images, we first considered classification using traditional machine learning methods. We used three traditional machine learning methods (KNN, SVM, Random Forest) for testing and analyzing the final results.\n",
    "Here is the core code for all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c272e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "file_path = ''\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assume spectrogram file path and format\n",
    "spectrogram_path = ''\n",
    "\n",
    "# Function to load spectrogram images\n",
    "def load_spectrogram(file_name):\n",
    "    img_path = os.path.join(spectrogram_path, file_name.replace('.wav', '.png'))\n",
    "    img = load_img(img_path, color_mode='grayscale') \n",
    "    img_array = img_to_array(img)\n",
    "    return img_array\n",
    "\n",
    "# Prepare dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    spectrogram = load_spectrogram(row['filename'])\n",
    "    X.append(spectrogram)\n",
    "    y.append(row['target'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Flatten the data into one-dimensional vectors\n",
    "X_flatten = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Split data into training and testing sets, using stratified splitting to ensure uniform distribution of each class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flatten, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870804de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b30aa",
   "metadata": {},
   "source": [
    "#### 4.1.2 Running Result\n",
    "Below is a table of results for all test data\n",
    "\n",
    "<img src=\"./picture/KNN.png\" alt=\"final\" width=\"600\">\n",
    "\n",
    "In traditional machine learning approaches, we experimented with three models: KNN, SVM, and Random Forest. We also tried various data preprocessing and testing methods, such as different image sizes (1000x400, 224x224) and different train-test splits (90:10, 80:20, 70:30). We found that the best performance was achieved with an image size of 1000x400 and a train-test split of 90:10. Among these three models, the highest accuracy rates were 27% for KNN, 38.75% for SVM, and 40% for Random Forest. Despite these efforts, the overall performance was unsatisfactory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc662",
   "metadata": {},
   "source": [
    "After analyzing the test results of the above three traditional machine learning, we found that they are not very effective in classifying audio images. After our analysis, we feel that it is mainly because of their limitations in feature extraction, handling high dimensional data, exploiting temporal dependencies, and large-scale data processing capabilities. \n",
    "\n",
    "In contrast, deep learning methods have significant advantages in these areas and can perform much better in audio image classification tasks. So in our next work, we consider comparative tests using different deep learning models in the hope of finding the one that performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29badb8",
   "metadata": {},
   "source": [
    "#### 4.2 CNN\n",
    "We analyze and compare their test performance by reviewing relevant audio image classification papers, we consider the following deep learning models, mainly considering convolutional neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b24e4",
   "metadata": {},
   "source": [
    "##### 4.2.1 AlexNet\n",
    "AlexNet is characterized by its innovative network architecture and combination of techniques, including ReLU activation function, Dropout regularization, overlap pooling, data augmentation, and parallel computing, which can effectively improve the accuracy of the test.\n",
    "\n",
    "*Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2020). Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery, 34(6), 1936-1962.*\n",
    "\n",
    "For the AlexNet model we conducted several tests using control variables in terms of feature extraction, image size, optimizer, number of training rounds, etc. and plotted the final test results as the table below.\n",
    "\n",
    "<img src=\"./picture/截屏2024-07-23 21.38.10.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "By analyzing the results of several tests, we found that the accuracy can reach up to 59.5%. \n",
    "\n",
    "The AlexNet model reads and preprocesses the spectrogram image, resizes it to 224x224 pixels, and normalizes it. Data enhancement was performed using ImageDataGenerator, which includes operations such as rotation, translation and horizontal flipping. The defined AlexNet model consists of five convolutional layers and three fully connected layers using ReLU activation function and L2 regularization. The model is compiled using the SGD optimizer and trained by the fit method, during which the Dropout technique is applied to prevent overfitting. \n",
    "\n",
    "Finally, the performance of the model is evaluated by the test data, and the classification report and confusion matrix are generated for visualization and analysis.\n",
    "\n",
    "Below is the relevant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c483ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Read CSV file\n",
    "file_path = ''\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the spectrogram file path and file format\n",
    "spectrogram_path = ''\n",
    "\n",
    "# Function: Load spectrogram image\n",
    "def load_spectrogram(file_name):\n",
    "    img_path = os.path.join(spectrogram_path, file_name.replace('.wav', '.png'))\n",
    "    img = load_img(img_path, target_size=(224, 224))  # AlexNet's default input size is 224x224\n",
    "    img_array = img_to_array(img)\n",
    "    img_array /= 255.0  # Normalize image\n",
    "    return img_array\n",
    "\n",
    "# Prepare the dataset\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    spectrogram = load_spectrogram(row['filename'])\n",
    "    X.append(spectrogram)\n",
    "    y.append(row['target'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_categorical = to_categorical(y, num_classes=len(df['target'].unique()))\n",
    "\n",
    "# Split data into training and testing sets using stratified split to ensure even distribution of each class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Define the AlexNet model\n",
    "def create_alexnet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First convolutional and pooling layer\n",
    "    model.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.001)))\n",
    "    model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Second convolutional and pooling layer\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Third convolutional layer\n",
    "    model.add(Conv2D(384, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "    # Fourth convolutional layer\n",
    "    model.add(Conv2D(384, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "    # Fifth convolutional and pooling layer\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "    # Flatten and fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(df['target'].unique())\n",
    "model = create_alexnet(input_shape, num_classes)\n",
    "\n",
    "# Adjust learning rate and momentum\n",
    "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=80, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07583f1",
   "metadata": {},
   "source": [
    "##### 4.2.2 VGG16\n",
    "VGG is a network structure with sixteen layers deep. Due to its use of the same convolutional kernel and pooling kernel, as well as its unified design, the architecture of VGG16 is relatively simple and easy to understand and implement, and it also has an excellent performance in dealing with the problem of image classification, which is why we have chosen it as one of our basic models.\n",
    "\n",
    "*Qassim, H., Verma, A., & Feinzimer, D. (2018, January). Compressed residual-VGG16 CNN model for big data places image recognition. In 2018 IEEE 8th annual computing and communication workshop and conference (CCWC) (pp. 169-175). IEEE.*\n",
    "\n",
    "For the VGG16 model we conducted several tests using control variables in terms of feature extraction, image size, optimizer, number of training rounds, etc. and plotted the final test results as the table below.\n",
    "\n",
    "<img src=\"./picture/截屏2024-07-23 10.28.31.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "By analyzing the results of several tests, we found that the model can achieve up to 60% accuracy. The model reads and preprocesses the spectrogram image, resizing it to 224x224 pixels and normalizing it. The defined VGG16 model consists of a pre-trained VGG16 convolutional layer (without the top fully connected layer) and a custom fully connected layer using the ReLU activation function and L2 regularization. The model is compiled using the Adam optimizer and trained by the fit method, during which the Dropout technique is applied to prevent overfitting. Finally, the model performance is evaluated by test data.\n",
    "\n",
    "The core code is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15edfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2  # Import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the convolutional layer weights of VGG16\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the new model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(4096, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(df['target'].unique()), activation='softmax'))\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5749efb",
   "metadata": {},
   "source": [
    "##### 4.2.3 ResNet\n",
    "ResNet50 is a network sturcture with 50 deep convolutional neural network. This network was introduced residual blocks to train the deeper netural network. And it is widely used in image classification tasks.\n",
    "\n",
    "*He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).*\n",
    "\n",
    "In this project, we alse used Global average pooling layer, fully connected layer, Dropout layer and output layer based on ResNet50 model to further process the features, prevent overfitting and output results.\n",
    "\n",
    "This code is used for classification task of spectrogram. Firstly, the spectrogram file name and corresponding labels are read from the CSV file and the spectrogram image is loaded and preprocessed and the dataset is divided into training set and test set according to the classification. Next, the code uses the ResNet50 model as a feature extractor to construct a new classification model and train it on the training set while evaluating it on the test set. \n",
    "\n",
    "After the training of this code is completed, the code calculates and prints the accuracy and F1 scores of the model, plots the curves of training and validation losses, and displays the confusion matrix and the accuracy of each category, and calculates and prints the running time of the whole process.\n",
    "\n",
    "<img src=\"./picture/Resnet.png\" alt=final width=\"1000\">\n",
    "\n",
    "For the ResNet50 model, we tried to use three ratios, 9:1, 8:2, and 7:3, to partition the dataset into a training set and a test set, and all other things being equal, the 9:1 training-test ratio was able to have the best accuracy (77%) and F1-score (0.767). For the 9:1 training-to-test ratio, we used three optimizers, Adam, Adagrad, and SGD, with different learning rates, and after experimentation, the Adam optimizer worked better for the model.\n",
    "\n",
    "The following is the core code of ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ResNet base model\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the convolutional layers of ResNet\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build a new model on top of ResNet\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "x = resnet_base(input_tensor, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(len(df['target'].unique()), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0263d28",
   "metadata": {},
   "source": [
    "##### 4.2.4 DenseNet\n",
    "DenseNet121 connects each layerto every other layer in a feed-forward fashion.Foreach layer, the feature-maps of all preceding layers areused as inputs, and its own feature-maps are used as inputsinto all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n",
    "\n",
    "*Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).*\n",
    "\n",
    "In this project, we alse used Global average pooling layer, fully connected layer, Dropout layer and output layer based on DenseNet121 model to further process the features, prevent overfitting and output results.\n",
    "\n",
    "This code is used for classification task of spectrogram. Firstly, the spectrogram file name and corresponding labels are read from the CSV file and the spectrogram image is loaded and preprocessed and the dataset is divided into training set and test set according to the classification. Next, the code uses the DenseNet121 model as a feature extractor to construct a new classification model and train it on the training set while evaluating it on the test set. After the training of this code is completed, the code calculates and prints the accuracy and F1 score of the model, plots the curves of training and validation losses, and displays the confusion matrix and the accuracy of each category, and calculates and prints the running time of the whole process.\n",
    "\n",
    "<img src=\"./picture/Densnet.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "For the DenseNet121 model, we tried to use three ratios, 9:1, 8:2, and 7:3, to partition the dataset into a training set and a test set, and all other things being equal, the 9:1 training-test ratio was able to have the best accuracy (80.5%) and F1-score (0.799). For the 9:1 training-to-test ratio, we used three optimizers, Adam, Adagrad, and SGD, with different learning rates, and after experimentation, the Adam optimizer worked better for the model.\n",
    "\n",
    "The following is the core code of DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b45659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DenseNet pre-processing model\n",
    "densenet_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the convolutional layer weights of DenseNet\n",
    "for layer in densenet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build a new model\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "x = densenet_base(input_tensor, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(len(df['target'].unique()), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66484caa",
   "metadata": {},
   "source": [
    "#### 4.3 RNN Model\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that are specialized for processing sequential data. Unlike Convolutional Neural Networks (CNNs), which are good at processing grid-like data such as images, RNNs are particularly well suited for tasks involving time series or sequence data.\n",
    "\n",
    "RNNs have several unique advantages in audio classification tasks:\n",
    "\n",
    "1. **Direct processing of sequential data**: RNNs can directly process the temporal characteristics of audio signals without first converting them to spectrograms or other image-like representations like CNNs.\n",
    "\n",
    "2. **Capture long-term dependencies**: RNNs are able to maintain internal states, allowing them to capture long-term dependencies in audio data. This is particularly useful for understanding context in sounds that evolve over time.\n",
    "\n",
    "3. **Variable input length**: RNNs can process input sequences of varying lengths, which is useful when processing audio clips of varying lengths.\n",
    "\n",
    "4. **Memory efficiency**: For long sequences, RNNs are more memory-efficient than CNNs because they process inputs sequentially rather than as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69442cfd",
   "metadata": {},
   "source": [
    "##### 4.3.1 LSTM (Long Short-Term Memory)\n",
    "\n",
    "Long Short-Term Memory *(Hochreiter & Schmidhuber, 1997)* is a special type of RNN designed to solve the vanishing gradient problem that standard RNNs may face when processing long sequences. LSTM introduces a memory cell and various gates (input, forget, and output gates) that enable the network to selectively remember or forget information in long sequences.\n",
    "\n",
    "*Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.*\n",
    "\n",
    "Our implementation of the LSTM model includes several key components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_rnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946ebf6",
   "metadata": {},
   "source": [
    "Key features of our model:\n",
    "\n",
    "1. **Stacked LSTM layers**: We use two LSTM layers, allowing the model to learn hierarchical representations of the audio data.\n",
    "\n",
    "2. **Regularization**: We apply L2 regularization to the LSTM and Dense layers to prevent overfitting.\n",
    "\n",
    "3. **Batch Normalization**: After each LSTM layer, we include a BatchNormalization layer to stabilize the learning process and potentially allow for higher learning rates.\n",
    "\n",
    "4. **Dropout**: We incorporate Dropout layers after the Dense layers to further combat overfitting.\n",
    "\n",
    "5. **Dense layers**: The model includes two Dense layers with ReLU activation before the final classification layer, allowing for non-linear combinations of the learned features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815aa5a4",
   "metadata": {},
   "source": [
    "##### 4.3.2 Bi-LSTM\n",
    "\n",
    "To further improve performance, we implemented a bidirectional LSTM (Bi-LSTM) model *(Schuster & Paliwal, 1997)*. Bi-LSTM processes the input sequence in both forward and backward directions, allowing the network to capture information that may come from future context as well as past context.\n",
    "\n",
    "*Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11), 2673-2681.*\n",
    "\n",
    "Advantages of Bi-LSTM over standard LSTM include:\n",
    "\n",
    "1. **Improved contextual understanding**: By processing the sequence bidirectionally, Bi-LSTM can capture more comprehensive context for each time step in the audio.\n",
    "\n",
    "2. **Better performance on tasks that require full sequence context**: For the audio datasets we use, Bi-LSTM generally outperforms unidirectional LSTM because they are correlated throughout the sequence.\n",
    "\n",
    "3. **Increased model capacity**: Bi-LSTM effectively doubles the number of parameters in the recurrent layer, potentially allowing for more complex feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_improved_rnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        BatchNormalization(),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d486d",
   "metadata": {},
   "source": [
    "##### 4.3.3 Augmentation\n",
    "\n",
    "To further enhance our model's performance and generalization capabilities, we implemented data augmentation techniques specifically tailored for audio data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf237fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio, sr):\n",
    "    # Time shift\n",
    "    shift_max = int(sr * 0.1)\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    augmented = np.roll(audio, shift)\n",
    "\n",
    "    # Add noise\n",
    "    noise_factor = 0.05\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented += noise_factor * noise\n",
    "\n",
    "    # Normalize\n",
    "    augmented = np.clip(augmented, -1, 1)\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83d011",
   "metadata": {},
   "source": [
    "Our augmentation strategy consists of two main techniques:\n",
    "\n",
    "1. **Time Shift**: We randomly shift the audio time by up to 10% of its length. This helps the model remain invariant to the exact timing of the sounds in the clip.\n",
    "\n",
    "2. **Noise Injection**: We add a small amount of random noise to the audio. This simulates real-world conditions where background noise is often present and helps the model be more robust to such changes.\n",
    "\n",
    "3. **Normalization**: After applying these augmentations, we clip the values ​​to ensure they stay within the valid range of the audio data.\n",
    "\n",
    "These augmentation techniques help artificially increase the diversity of our training data, potentially improving the model's ability to generalize to new, unseen audio samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fad28",
   "metadata": {},
   "source": [
    "We summarize the results of several of the above runs with respect to the LSTM model in the following table:\n",
    "\n",
    "<img src=\"./picture/LSTM结果.jpg\" alt=\"final\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7baac",
   "metadata": {},
   "source": [
    "##### 4.3.4 GRU\n",
    "GRU stands for Gated Recurrent Unit, which is a type of recurrent neural network (RNN) architecture and a variant of the Long Short-Term Memory (LSTM) network. GRUs are good at handle sequence data and capture temporal dependencies without suffering from the vanishing gradient problem that can affect traditional RNNs.\n",
    "\n",
    "This model uses the 'Sequential' model from TensorFlow's Keras API and includes 'Input' and 'GRU' layers from Keras. In the GRU layer, we define 64 units. The 'Dense' layer uses 'relu' (Rectified Linear Unit) as the activation function. The 'Dropout' layer sets 50% of the neurons to drop out during training. The final 'Dense' layer uses the 'softmax' activation function, which is commonly used for multi-class classification problems. In the 'compile' method, we choose the 'adam' optimizer, known for achieving better accuracy.\n",
    "\n",
    "*Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53912905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple GRU model\n",
    "def create_simple_gru_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        GRU(64),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97edade3",
   "metadata": {},
   "source": [
    "##### 4.3.5 Bi-GRU\n",
    "Bi-GRU processes sequences in both forward and backward directions, allowing it to achieve better performance in audio classification where context from both directions is important. Additionally, it is more robust to data variations such as noise and differing sequence lengths. By capturing dependencies missed by unidirectional GRUs, Bi-GRU leads to higher accuracy and richer feature extraction.\n",
    "\n",
    "For the Bidirectional GRU model, we use the 'Bidirectional' wrapper along with the GRU layer from Keras. The 'Dense' layers use the same 'relu' and 'softmax' activation functions. We retain the 'adam' optimizer and the same loss function in the 'compile' method from Keras.\n",
    "\n",
    "*Li, X., Ma, X., Xiao, F., Xiao, C., Wang, F., & Zhang, S. (2022). Time-series production forecasting method based on the integration of Bidirectional Gated Recurrent Unit (Bi-GRU) network and Sparrow Search Algorithm (SSA). Journal of Petroleum Science and Engineering, 208, 109309.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bidirectional GRU model\n",
    "def create_simple_gru_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(GRU(64)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41a2ef",
   "metadata": {},
   "source": [
    "In this section, we used the same data augmentation methods as in the LSTM section: time shift, noise addition, and normalization. Additionally, we experimented with different feature extraction methods, including MFCC and Mel spectrograms. The results are as follows.\n",
    "\n",
    "<img src=\"./picture/GRU.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "As can be seen from the results, with the introduction of GRU-related modeling and data augmentation, we get the highest accuracy of 78.5% in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83f0fc",
   "metadata": {},
   "source": [
    "### 5 Basic Model Enhancement and Result\n",
    "\n",
    "#### 5.1  Basic model results\n",
    "\n",
    "These are all the basic models we have chosen, and we have selected the best performing cases of each basic model to be plotted in the table below.\n",
    "\n",
    "<img src=\"./picture/final.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "In different base models, we experimented with various image sizes (1000x400, 224x224), different training/testing splits (9:1, 8:2, 7:3), different optimizers (Adam, SGD, Adagrad), and different learning rates (0.01, 0.001, 0.0001). Based on these analyses, we compiled a table showing the highest accuracy achieved for each model along with the corresponding image size, training/testing split, optimizer, and learning rate. The standout model among machine learning, CNN, and RNN approaches was the DenseNet model from CNN, achieving an accuracy of 80.5%. The next best was the Bi-GRU model from RNN, which achieved an accuracy of 78.5% after data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47941f",
   "metadata": {},
   "source": [
    "#### 5.1  Analysis of baseline results\n",
    "\n",
    "Since we have chosen DenseNet as our base model, the next step is to consider how to enhance on this base model. We output the confusion matrix of the DenseNet model results during training as shown below.\n",
    "\n",
    "<img src=\"./picture/混淆矩阵.png\" alt=\"final\" width=\"500\">\n",
    "\n",
    "In the confusion matrix, we have labeled some categories with relatively low accuracy, we looked at the spectral images of these categories in the hope of finding the reason for the low accuracy, some of the categories are shown in the figure below.\n",
    "\n",
    "<img src=\"./picture/表现不好的类别.png\" alt=\"final\" width=\"500\">\n",
    "\n",
    "By analyzing these images, we found that some of the poorly performing categories shared the common feature of different repetitions of the sound in that category, of varying lengths. For example, for one of the categories that had sound clips about breathing, some of the audios had multiple breaths performed, while others had only one breath performed. So we feel that this could be the cause of the lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee16409",
   "metadata": {},
   "source": [
    "#### 5.2  Base model improvement enhancements\n",
    "\n",
    "To address the above problems, we believe that we can try to use data enhancement approaches, for example, we can enhance the training images by panning, adding noise, rotating, etc. to help the model better recognize different audio features and thus improve the classification performance.\n",
    "\n",
    "However, the data enhancement approach to the pictures is not effective in improving the DenseNet121 model in this project. After analysis, we believe that the spectrogram is a conversion of the audio signal into a visual representation too much visual enhancement of the spectrogram may destroy the inherent structure of the audio signal, resulting in the model not being able to understand the features of the audio correctly.\n",
    "\n",
    "Considering that in the base model we have better data enhancement results of the audio directly to the RNN structure, we decided to adopt the RNN structure mentioned above to use the data enhancement technique of the Bi-GRU model combined with the DenseNet121 model. Eventually there is some improvement for the testing accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb75bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def augment_audio(audio, sr):\n",
    "    # Time shift\n",
    "    shift_max = int(sr * 0.1)\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    augmented = np.roll(audio, shift)\n",
    "\n",
    "    # Add noise\n",
    "    noise_factor = 0.05\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented += noise_factor * noise\n",
    "\n",
    "    # Normalize\n",
    "    augmented = np.clip(augmented, -1, 1)\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(audio_dir, csv_file, max_length=220500, augment=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = os.path.join(audio_dir, row['filename'])\n",
    "        audio, sr = librosa.load(file_path, sr=44100, duration=5.0)\n",
    "        if augment:\n",
    "            audio = augment_audio(audio, sr)\n",
    "        if len(audio) < max_length:\n",
    "            audio = np.pad(audio, (0, max_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:max_length]\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        X.append(mel_spectrogram_db.T)\n",
    "        y.append(row['target'])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Bidirectional GRU model with DenseNet121 and Attention\n",
    "def create_birnn_gru_model_with_densenet_attention(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    reshaped_input = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(input_layer)\n",
    "    \n",
    "    # DenseNet121 model\n",
    "    densenet = DenseNet121(include_top=False, input_shape=(input_shape[0], input_shape[1], 1), weights=None)\n",
    "    densenet_out = densenet(reshaped_input)\n",
    "    densenet_out = tf.keras.layers.GlobalAveragePooling2D()(densenet_out)\n",
    "    \n",
    "    reshaped_densenet_out = tf.keras.layers.Reshape((1, -1))(densenet_out)\n",
    "    birnn_out = Bidirectional(GRU(64, return_sequences=True))(reshaped_densenet_out)\n",
    "    attention_out = Attention()(birnn_out)\n",
    "    \n",
    "    dense1 = Dense(64, activation='relu')(attention_out)\n",
    "    dropout = Dropout(0.5)(dense1)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528e035",
   "metadata": {},
   "source": [
    "For the above model, after reviewing information and papers, we try to introduce the attention mechanism to optimize the model. We used the additive attention mechanism, which is used on the bidirectional GRU in this model, to focus on the output of the time step of the GRU, enabling the model to focus on more important features. By adding the attention mechanism, the model is able to focus on important time steps in the input sequence to improve the model's audio classification ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d406e",
   "metadata": {},
   "source": [
    "For several scenarios mentioned above for the basic model enhancement, we have tested them, and the relevant tests belong to the following figure.\n",
    "\n",
    "<img src=\"./picture/improve结果表格.png\" alt=\"final\" width=\"1000\">\n",
    "\n",
    "By analyzing the data results, we can find that the best test results can be obtained by using the Bi-GRU model with data enhancement techniques in combination with the DenseNet121 model followed by the introduction of the additive attention mechanism, with a test result of 87%.\n",
    "\n",
    "We use this model as our final result after lifting on the base model, and the complete code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33a8e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 5s/step - accuracy: 0.0283 - loss: 3.9323 - val_accuracy: 0.0237 - val_loss: 3.9265 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 4s/step - accuracy: 0.0479 - loss: 3.8229 - val_accuracy: 0.0211 - val_loss: 4.3189 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 5s/step - accuracy: 0.0522 - loss: 3.6988 - val_accuracy: 0.0211 - val_loss: 4.0221 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 5s/step - accuracy: 0.0989 - loss: 3.4716 - val_accuracy: 0.0316 - val_loss: 4.3279 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 5s/step - accuracy: 0.1028 - loss: 3.2969 - val_accuracy: 0.0605 - val_loss: 3.8046 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 4s/step - accuracy: 0.1166 - loss: 3.1770 - val_accuracy: 0.0632 - val_loss: 3.3808 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4s/step - accuracy: 0.1259 - loss: 3.0566 - val_accuracy: 0.1105 - val_loss: 3.3906 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 4s/step - accuracy: 0.1575 - loss: 2.9432 - val_accuracy: 0.0711 - val_loss: 4.1960 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 5s/step - accuracy: 0.1814 - loss: 2.8205 - val_accuracy: 0.1079 - val_loss: 3.3706 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 5s/step - accuracy: 0.2157 - loss: 2.6824 - val_accuracy: 0.0263 - val_loss: 5.1918 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 5s/step - accuracy: 0.2648 - loss: 2.5531 - val_accuracy: 0.1895 - val_loss: 2.9793 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 4s/step - accuracy: 0.2935 - loss: 2.4241 - val_accuracy: 0.1500 - val_loss: 3.2736 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 4s/step - accuracy: 0.3148 - loss: 2.3223 - val_accuracy: 0.3395 - val_loss: 2.3792 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 4s/step - accuracy: 0.3448 - loss: 2.1747 - val_accuracy: 0.3132 - val_loss: 2.2559 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 4s/step - accuracy: 0.3814 - loss: 2.0889 - val_accuracy: 0.3211 - val_loss: 2.2339 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 4s/step - accuracy: 0.3964 - loss: 1.9895 - val_accuracy: 0.3289 - val_loss: 2.2912 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 4s/step - accuracy: 0.4089 - loss: 1.9254 - val_accuracy: 0.2684 - val_loss: 2.4218 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 4s/step - accuracy: 0.4455 - loss: 1.8077 - val_accuracy: 0.3395 - val_loss: 2.2191 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 4s/step - accuracy: 0.4671 - loss: 1.7556 - val_accuracy: 0.4632 - val_loss: 1.7767 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 4s/step - accuracy: 0.4922 - loss: 1.6653 - val_accuracy: 0.3342 - val_loss: 2.1571 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 4s/step - accuracy: 0.4986 - loss: 1.6289 - val_accuracy: 0.4316 - val_loss: 1.9283 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 4s/step - accuracy: 0.5441 - loss: 1.5100 - val_accuracy: 0.4395 - val_loss: 1.8126 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 4s/step - accuracy: 0.5334 - loss: 1.5002 - val_accuracy: 0.4947 - val_loss: 1.5000 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 4s/step - accuracy: 0.5518 - loss: 1.4480 - val_accuracy: 0.4211 - val_loss: 2.0455 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 4s/step - accuracy: 0.5643 - loss: 1.3858 - val_accuracy: 0.5053 - val_loss: 1.4961 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 4s/step - accuracy: 0.6180 - loss: 1.2684 - val_accuracy: 0.5526 - val_loss: 1.3826 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 4s/step - accuracy: 0.6120 - loss: 1.2224 - val_accuracy: 0.4974 - val_loss: 1.7547 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 4s/step - accuracy: 0.6299 - loss: 1.1765 - val_accuracy: 0.3421 - val_loss: 2.7808 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 4s/step - accuracy: 0.6469 - loss: 1.1478 - val_accuracy: 0.6263 - val_loss: 1.2101 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 5s/step - accuracy: 0.6616 - loss: 1.0912 - val_accuracy: 0.5000 - val_loss: 1.6341 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 4s/step - accuracy: 0.6607 - loss: 1.0503 - val_accuracy: 0.5974 - val_loss: 1.3062 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2135s\u001b[0m 20s/step - accuracy: 0.7134 - loss: 0.9218 - val_accuracy: 0.5895 - val_loss: 1.3781 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 7s/step - accuracy: 0.6966 - loss: 1.0144 - val_accuracy: 0.5000 - val_loss: 1.7331 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 5s/step - accuracy: 0.7080 - loss: 0.9394 - val_accuracy: 0.6395 - val_loss: 1.2716 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 5s/step - accuracy: 0.7741 - loss: 0.7317 - val_accuracy: 0.7026 - val_loss: 0.8744 - learning_rate: 2.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 5s/step - accuracy: 0.8190 - loss: 0.6096 - val_accuracy: 0.6816 - val_loss: 0.9009 - learning_rate: 2.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2528s\u001b[0m 24s/step - accuracy: 0.8387 - loss: 0.5428 - val_accuracy: 0.7237 - val_loss: 0.8562 - learning_rate: 2.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4449s\u001b[0m 42s/step - accuracy: 0.8388 - loss: 0.5316 - val_accuracy: 0.6342 - val_loss: 1.1118 - learning_rate: 2.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1607s\u001b[0m 15s/step - accuracy: 0.8356 - loss: 0.5420 - val_accuracy: 0.7132 - val_loss: 0.8790 - learning_rate: 2.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1471s\u001b[0m 14s/step - accuracy: 0.8501 - loss: 0.4786 - val_accuracy: 0.7211 - val_loss: 0.9124 - learning_rate: 2.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3120s\u001b[0m 29s/step - accuracy: 0.8392 - loss: 0.5102 - val_accuracy: 0.7474 - val_loss: 0.8028 - learning_rate: 2.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2438s\u001b[0m 23s/step - accuracy: 0.8450 - loss: 0.4790 - val_accuracy: 0.7105 - val_loss: 0.8510 - learning_rate: 2.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2240s\u001b[0m 21s/step - accuracy: 0.8551 - loss: 0.4538 - val_accuracy: 0.7289 - val_loss: 0.9108 - learning_rate: 2.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1433s\u001b[0m 5s/step - accuracy: 0.8665 - loss: 0.4419 - val_accuracy: 0.7526 - val_loss: 0.7950 - learning_rate: 2.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 5s/step - accuracy: 0.8812 - loss: 0.4135 - val_accuracy: 0.7158 - val_loss: 0.9140 - learning_rate: 2.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1451s\u001b[0m 14s/step - accuracy: 0.8850 - loss: 0.3930 - val_accuracy: 0.7368 - val_loss: 0.8346 - learning_rate: 2.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1125s\u001b[0m 11s/step - accuracy: 0.8884 - loss: 0.4171 - val_accuracy: 0.7500 - val_loss: 0.8013 - learning_rate: 2.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 5s/step - accuracy: 0.8891 - loss: 0.3829 - val_accuracy: 0.7421 - val_loss: 0.8969 - learning_rate: 2.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1044s\u001b[0m 10s/step - accuracy: 0.9052 - loss: 0.3540 - val_accuracy: 0.7579 - val_loss: 0.8522 - learning_rate: 2.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1613s\u001b[0m 15s/step - accuracy: 0.9109 - loss: 0.3148 - val_accuracy: 0.7816 - val_loss: 0.7457 - learning_rate: 4.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2075s\u001b[0m 19s/step - accuracy: 0.9119 - loss: 0.3024 - val_accuracy: 0.7789 - val_loss: 0.7167 - learning_rate: 4.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 4s/step - accuracy: 0.9163 - loss: 0.3059 - val_accuracy: 0.7921 - val_loss: 0.7241 - learning_rate: 4.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1181s\u001b[0m 11s/step - accuracy: 0.9147 - loss: 0.3194 - val_accuracy: 0.7921 - val_loss: 0.7131 - learning_rate: 4.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 6s/step - accuracy: 0.9200 - loss: 0.2799 - val_accuracy: 0.7868 - val_loss: 0.7430 - learning_rate: 4.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 5s/step - accuracy: 0.9151 - loss: 0.3030 - val_accuracy: 0.7895 - val_loss: 0.7506 - learning_rate: 4.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.9196 - loss: 0.2765 - val_accuracy: 0.7947 - val_loss: 0.7147 - learning_rate: 4.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 5s/step - accuracy: 0.9160 - loss: 0.2794 - val_accuracy: 0.8079 - val_loss: 0.7212 - learning_rate: 4.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 5s/step - accuracy: 0.9238 - loss: 0.2707 - val_accuracy: 0.8026 - val_loss: 0.7138 - learning_rate: 4.0000e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 5s/step - accuracy: 0.9287 - loss: 0.2644 - val_accuracy: 0.8000 - val_loss: 0.7103 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 5s/step - accuracy: 0.9049 - loss: 0.3181 - val_accuracy: 0.7974 - val_loss: 0.7166 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 5s/step - accuracy: 0.9189 - loss: 0.2767 - val_accuracy: 0.7974 - val_loss: 0.7133 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 5s/step - accuracy: 0.9155 - loss: 0.2929 - val_accuracy: 0.8026 - val_loss: 0.7041 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 4s/step - accuracy: 0.9167 - loss: 0.2959 - val_accuracy: 0.8053 - val_loss: 0.7061 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 5s/step - accuracy: 0.9154 - loss: 0.2988 - val_accuracy: 0.8053 - val_loss: 0.7005 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.9281 - loss: 0.2601 - val_accuracy: 0.8026 - val_loss: 0.7017 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 4s/step - accuracy: 0.9321 - loss: 0.2448 - val_accuracy: 0.8000 - val_loss: 0.7065 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 5s/step - accuracy: 0.9297 - loss: 0.2556 - val_accuracy: 0.7947 - val_loss: 0.7079 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 5s/step - accuracy: 0.9209 - loss: 0.2764 - val_accuracy: 0.7921 - val_loss: 0.7178 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 6s/step - accuracy: 0.9264 - loss: 0.2804 - val_accuracy: 0.7921 - val_loss: 0.7125 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3532s\u001b[0m 33s/step - accuracy: 0.9254 - loss: 0.2663 - val_accuracy: 0.8000 - val_loss: 0.7097 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2439s\u001b[0m 23s/step - accuracy: 0.9318 - loss: 0.2642 - val_accuracy: 0.8053 - val_loss: 0.6993 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2925s\u001b[0m 28s/step - accuracy: 0.9205 - loss: 0.2721 - val_accuracy: 0.8026 - val_loss: 0.6961 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4s/step - accuracy: 0.9250 - loss: 0.2657 - val_accuracy: 0.7974 - val_loss: 0.7111 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 5s/step - accuracy: 0.9204 - loss: 0.2637 - val_accuracy: 0.7921 - val_loss: 0.7104 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 5s/step - accuracy: 0.9187 - loss: 0.2623 - val_accuracy: 0.7921 - val_loss: 0.7066 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2174s\u001b[0m 20s/step - accuracy: 0.9271 - loss: 0.2613 - val_accuracy: 0.7974 - val_loss: 0.7042 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 4s/step - accuracy: 0.9245 - loss: 0.2633 - val_accuracy: 0.8026 - val_loss: 0.7072 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 4s/step - accuracy: 0.9272 - loss: 0.2524 - val_accuracy: 0.8026 - val_loss: 0.7101 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 4s/step - accuracy: 0.9291 - loss: 0.2559 - val_accuracy: 0.8053 - val_loss: 0.7153 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 4s/step - accuracy: 0.9283 - loss: 0.2558 - val_accuracy: 0.8079 - val_loss: 0.7158 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1199s\u001b[0m 11s/step - accuracy: 0.9223 - loss: 0.2739 - val_accuracy: 0.8105 - val_loss: 0.7041 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2105s\u001b[0m 20s/step - accuracy: 0.9304 - loss: 0.2387 - val_accuracy: 0.8105 - val_loss: 0.7042 - learning_rate: 1.0000e-05\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 822ms/step\n",
      "Overall Accuracy: 0.8700\n",
      "Overall F1 Score: 0.8667\n",
      "Class 0 Accuracy: 1.0000\n",
      "Class 1 Accuracy: 0.7500\n",
      "Class 2 Accuracy: 1.0000\n",
      "Class 3 Accuracy: 0.7500\n",
      "Class 4 Accuracy: 1.0000\n",
      "Class 5 Accuracy: 0.7500\n",
      "Class 6 Accuracy: 1.0000\n",
      "Class 7 Accuracy: 1.0000\n",
      "Class 8 Accuracy: 1.0000\n",
      "Class 9 Accuracy: 1.0000\n",
      "Class 10 Accuracy: 1.0000\n",
      "Class 11 Accuracy: 1.0000\n",
      "Class 12 Accuracy: 1.0000\n",
      "Class 13 Accuracy: 1.0000\n",
      "Class 14 Accuracy: 1.0000\n",
      "Class 15 Accuracy: 0.5000\n",
      "Class 16 Accuracy: 0.7500\n",
      "Class 17 Accuracy: 1.0000\n",
      "Class 18 Accuracy: 0.7500\n",
      "Class 19 Accuracy: 1.0000\n",
      "Class 20 Accuracy: 1.0000\n",
      "Class 21 Accuracy: 1.0000\n",
      "Class 22 Accuracy: 1.0000\n",
      "Class 23 Accuracy: 0.2500\n",
      "Class 24 Accuracy: 1.0000\n",
      "Class 25 Accuracy: 1.0000\n",
      "Class 26 Accuracy: 1.0000\n",
      "Class 27 Accuracy: 0.7500\n",
      "Class 28 Accuracy: 0.7500\n",
      "Class 29 Accuracy: 0.7500\n",
      "Class 30 Accuracy: 1.0000\n",
      "Class 31 Accuracy: 0.7500\n",
      "Class 32 Accuracy: 0.7500\n",
      "Class 33 Accuracy: 0.7500\n",
      "Class 34 Accuracy: 0.7500\n",
      "Class 35 Accuracy: 0.5000\n",
      "Class 36 Accuracy: 1.0000\n",
      "Class 37 Accuracy: 1.0000\n",
      "Class 38 Accuracy: 1.0000\n",
      "Class 39 Accuracy: 0.7500\n",
      "Class 40 Accuracy: 0.5000\n",
      "Class 41 Accuracy: 0.7500\n",
      "Class 42 Accuracy: 1.0000\n",
      "Class 43 Accuracy: 1.0000\n",
      "Class 44 Accuracy: 1.0000\n",
      "Class 45 Accuracy: 1.0000\n",
      "Class 46 Accuracy: 1.0000\n",
      "Class 47 Accuracy: 0.7500\n",
      "Class 48 Accuracy: 0.5000\n",
      "Class 49 Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, GRU, Dense, Dropout, Layer\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_audio(audio, sr):\n",
    "    # Time shift\n",
    "    shift_max = int(sr * 0.1)\n",
    "    shift = np.random.randint(-shift_max, shift_max)\n",
    "    augmented = np.roll(audio, shift)\n",
    "\n",
    "    # Add noise\n",
    "    noise_factor = 0.05\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented += noise_factor * noise\n",
    "\n",
    "    # Normalize\n",
    "    augmented = np.clip(augmented, -1, 1)\n",
    "    return augmented\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(audio_dir, csv_file, max_length=220500, augment=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in df.iterrows():\n",
    "        file_path = os.path.join(audio_dir, row['filename'])\n",
    "        audio, sr = librosa.load(file_path, sr=44100, duration=5.0)\n",
    "        if augment:\n",
    "            audio = augment_audio(audio, sr)\n",
    "        if len(audio) < max_length:\n",
    "            audio = np.pad(audio, (0, max_length - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:max_length]\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        X.append(mel_spectrogram_db.T)\n",
    "        y.append(row['target'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define Attention Layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        score = tf.nn.tanh(tf.tensordot(x, self.W, axes=[-1, 0]) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * x\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# Define Bidirectional GRU model with DenseNet121 and Attention\n",
    "def create_birnn_gru_model_with_densenet_attention(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    reshaped_input = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(input_layer)\n",
    "    \n",
    "    # DenseNet121 model\n",
    "    densenet = DenseNet121(include_top=False, input_shape=(input_shape[0], input_shape[1], 1), weights=None)\n",
    "    densenet_out = densenet(reshaped_input)\n",
    "    densenet_out = tf.keras.layers.GlobalAveragePooling2D()(densenet_out)\n",
    "    \n",
    "    reshaped_densenet_out = tf.keras.layers.Reshape((1, -1))(densenet_out)\n",
    "    birnn_out = Bidirectional(GRU(64, return_sequences=True))(reshaped_densenet_out)\n",
    "    attention_out = Attention()(birnn_out)\n",
    "    \n",
    "    dense1 = Dense(64, activation='relu')(attention_out)\n",
    "    dropout = Dropout(0.5)(dense1)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# File paths\n",
    "audio_dir = 'audio'\n",
    "csv_file = 'meta/esc50.csv'\n",
    "\n",
    "# Load and preprocess data\n",
    "X, y = load_and_preprocess_data(audio_dir, csv_file)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "# Data augmentation for training set\n",
    "X_train_aug, y_train_aug = load_and_preprocess_data(audio_dir, csv_file, augment=True)\n",
    "X_train = np.concatenate([X_train, X_train_aug])\n",
    "y_train = np.concatenate([y_train, y_train_aug])\n",
    "\n",
    "# Create and train Bidirectional GRU model with DenseNet121 and Attention\n",
    "model_birnn_gru_densenet_attention = create_birnn_gru_model_with_densenet_attention(X_train.shape[1:], len(np.unique(y)))\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_birnn_gru_densenet_attention_model.keras', save_best_only=True, monitor='val_accuracy')\n",
    "\n",
    "history_birnn_gru_densenet_attention = model_birnn_gru_densenet_attention.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[reduce_lr, early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Bidirectional GRU model with DenseNet121 and Attention\n",
    "y_pred_birnn_gru_densenet_attention = model_birnn_gru_densenet_attention.predict(X_test)\n",
    "y_pred_classes_birnn_gru_densenet_attention = np.argmax(y_pred_birnn_gru_densenet_attention, axis=1)\n",
    "accuracy_birnn_gru_densenet_attention = accuracy_score(y_test, y_pred_classes_birnn_gru_densenet_attention)\n",
    "f1_birnn_gru_densenet_attention = f1_score(y_test, y_pred_classes_birnn_gru_densenet_attention, average='weighted')\n",
    "print(f'Overall Accuracy: {accuracy_birnn_gru_densenet_attention:.4f}')\n",
    "print(f'Overall F1 Score: {f1_birnn_gru_densenet_attention:.4f}')\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracies_birnn_gru_densenet_attention = {}\n",
    "for class_id in np.unique(y):\n",
    "    mask = y_test == class_id\n",
    "    class_accuracy = accuracy_score(y_test[mask], y_pred_classes_birnn_gru_densenet_attention[mask])\n",
    "    class_accuracies_birnn_gru_densenet_attention[class_id] = class_accuracy\n",
    "    print(f'Class {class_id} Accuracy: {class_accuracy:.4f}')\n",
    "\n",
    "# Plot learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_birnn_gru_densenet_attention.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_birnn_gru_densenet_attention.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_birnn_gru_densenet_attention.history['loss'], label='Training Loss')\n",
    "plt.plot(history_birnn_gru_densenet_attention.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves_birnn_gru_densenet_attention.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot confusion matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_birnn_gru_densenet_attention = confusion_matrix(y_test, y_pred_classes_birnn_gru_densenet_attention)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_birnn_gru_densenet_attention, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix_birnn_gru_densenet_attention.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot class accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(class_accuracies_birnn_gru_densenet_attention.keys(), class_accuracies_birnn_gru_densenet_attention.values())\n",
    "plt.title('Accuracy by Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('class_accuracies_birnn_gru_densenet_attention.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ebe6b",
   "metadata": {},
   "source": [
    "Plot confusion matrix\n",
    "\n",
    "<img src=\"./picture/wemeet image_20240804120902074.png\" alt=\"final\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6b40b",
   "metadata": {},
   "source": [
    "##### 6.Discution\n",
    "\n",
    "#### 6.1 Strengths\n",
    "+ **High-efficiency feature extraction:** High-efficiency feature extraction through DenseNet121, combined with Mel spectrogram, makes the feature extraction process more efficient and meaningful.\n",
    "\n",
    "+ **Timing Modeling Capability:** Enhanced timing modeling capability for audio data through bi-directional GRU modeling, which better captures the temporal dependencies in audio signals.\n",
    "\n",
    "+ **Enhanced model robustness:** Data enhancement strategies (time offset and additive noise) improve the robustness of the model to the diversity of audio data in real-world applications.\n",
    "\n",
    "+ **Optimizing the training process:** The use of callback functions optimizes the training process and improves the convergence speed and final performance of the model.\n",
    "\n",
    "+ **high accuracy：** The final model achieved a high level of accuracy (87%), with many categories improving in the final model compared to the base model.\n",
    "\n",
    "#### 6.2 Weaknesses\n",
    "In the 10 categories where the baseline DenseNet model achieved accuracies below 50%, our final model (Bi-GRU + DenseNet + data augmentation + attention mechanism) improved 7 of them. Among the 3 categories that did not see improvement, the most notable were categories 23 and 28. These categories, representing breathing and snoring sounds respectively, were confused in the final results, leading to lower accuracies.\n",
    "\n",
    "<img src=\"./picture/最终结果缺点.jpg\" alt=\"final\" width=\"800\">\n",
    "\n",
    "We analyzed the possible reasons for the model's failure to distinguish these two categories:\n",
    "\n",
    "+ **Visual Similarities**: The spectrograms for these sounds show similar frequency patterns and intensity distributions.\n",
    "+ **Acoustic Similarities**: Both sounds have similar timbre, pitch, and rhythmic patterns, making them difficult to differentiate.\n",
    "+ **Technical Limitations**: The DenseNet may struggle to make fine distinctions between these similar sounds. The Bi-GRU might also have difficulty capturing dependencies in both directions effectively. Even with an attention layer, if the key features of both classes are similar, the mechanism may not differentiate them well.\n",
    "\n",
    "#### 6.3 Future Work\n",
    "To improve the model, we suggest:\n",
    "\n",
    "+ **Increasing the Dataset Size**: Pretraining with a larger dataset could help the model learn more subtle differences between the categories.\n",
    "+ **Using More Advanced Models**: Employing more sophisticated, computationally intensive models and advanced data augmentation techniques might enhance the model's ability to distinguish between these similar sounds."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
